\subsection*{Вопрос 2}

\begin{condition}
	Цепи Маркова. Переходные вероятности. Матрица переходных вероятностей однородной цепи Маркова, её свойства. Примеры.
\end{condition}

Последовательность случайных величин $\{X_k\}_{k=0}^{\infty}$ со значениями в
$S=\{E_1,E_2,\ldots\}$ называется цепью Маркова, если выполняется равенство
условных вероятностей
$
	P\!\left(X_{i_k}=E_{j_k}\mid X_{i_1}=E_{j_1},\ldots,X_{i_{k-1}}=E_{j_{k-1}}\right)
	=
	P\!\left(X_{i_k}=E_{j_k}\mid X_{i_{k-1}}=E_{j_{k-1}}\right)
$
для произвольных $i_1<i_2<\ldots<i_{k-1}<i_k$, $(k=3,4,\ldots)$ и любых
$E_{j_1},\ldots,E_{j_k}$.

Если вероятности
$
	p_{ij}=P(X_{k+1}=E_j\mid X_k=E_i)
$
не зависят от $k$, то цепь Маркова называется однородной. При этом $p_{ij}$
называются переходными вероятностями, а матрица
$
	P=(p_{ij})
$
называется матрицей вероятностей перехода за один шаг или переходной матрицей.

В однородной цепи Маркова вероятности
$
	p_{ij}(m)=P(X_{k+m}=E_j\mid X_k=E_i)
$
тоже не зависят от $k$. Матрица
$
	P(m)=(p_{ij}(m))
$
называется матрицей вероятностей перехода за $m$ шагов. При этом
$
	0\le p_{ij}(m)\le 1,\qquad \sum_j p_{ij}(m)=1.
$
Матрицы с такими свойствами называются стохастическими.

Вектор
$
	\vec p(m)=(p_1(m),p_2(m),\ldots),
	\qquad
	p_i(m)=P(X_m=E_i),
$
называется вектором распределения вероятностей через $m$ шагов. При этом
$
	0\le p_i(m)\le 1,\qquad \sum_i p_i(m)=1.
$

Вектор
$
	\vec p(0)=(p_1(0),p_2(0),\ldots)
$
называется начальным распределением вероятностей цепи Маркова.

Пусть $\{X_k\}_{k=0}^{\infty}$ — однородная цепь Маркова. Тогда
$
	P(m)=P^m.
$

Пусть $\{X_k\}_{k=0}^{\infty}$ — однородная цепь Маркова. Тогда
$
	\vec p(m)=\vec p(0)\,P(m).
$

В качестве следствия получаем
$
	\vec p(m)=\vec p(0)\,P^m.
$

Зная $\vec p(0)$ и $P$, можно найти все конечномерные распределения цепи Маркова:
для произвольных $i_1<i_2<\ldots<i_k$ и любых $E_{j_1},\ldots,E_{j_k}$ имеем
$
	\begin{aligned}
		P(X_{i_1}=E_{j_1},\ldots,X_{i_k}=E_{j_k})
		 & =
		P(X_{i_1}=E_{j_1})
		P(X_{i_2}=E_{j_2}\mid X_{i_1}=E_{j_1})
		\cdots
		P(X_{i_k}=E_{j_k}\mid X_{i_{k-1}}=E_{j_{k-1}}) \\
		 & =
		p_{j_1}(i_1)\,
		p_{j_1j_2}(i_2-i_1)\cdots
		p_{j_{k-1}j_k}(i_k-i_{k-1}).
	\end{aligned}
$
